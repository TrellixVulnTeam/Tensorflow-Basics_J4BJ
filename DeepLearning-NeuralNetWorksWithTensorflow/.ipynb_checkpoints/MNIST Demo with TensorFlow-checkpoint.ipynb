{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network = Input Times Weight, add a bias, Activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining workflow\n",
    "### input > weight > hidden layer1 > activation function > weights > hidden layer 2 > activation function > weights > output (!yeah)\n",
    "### how close you are to the output, cost function, apply optimizer to minimize cost function, several to choose from, for example-- Gradient Descent, Adam Optimizer etc...\n",
    "### Backpropagation to update weights in order to minimize cost function\n",
    "### 1 Forward Propagation + 1 Backward Propagation. = 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the dataset, one hot = True sets up one hot encoding to True. There is a good explanation for OHE here:  \n",
    " https://goo.gl/Eyrhzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model, this can be anything\n",
    "nodes_hidden_layer_1 = 500\n",
    "nodes_hidden_layer_2 = 600\n",
    "nodes_hidden_layer_3 = 700\n",
    "nodes_hidden_layer_4 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have 10 classes for the classification here, from 0...9, therefore,\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to define the batch size, this means that it will do the whole computation in batches of 100,\n",
    "# Basically taking 100 features at a time.\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We have to define place holders for the input and output to run whole thing in session, 784 is 28*28 to \n",
    "define the shape of the image''' \n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Defining Neural Network\n",
    "Weights and Biases are tensorflow variable. Biases are added to Input times Variable, \n",
    "workflow is basically first hidden layer input will be the same as our input. \n",
    "However, from the second layer, the output of first will become input of second and so on\n",
    "and so forth. After last hidden layer, output we desire to have a class, so that is where num_classes \n",
    "comes into play\n",
    "'''\n",
    "\n",
    "def neural_net(Dataset_input):\n",
    "    \n",
    "    hidden_layer_1 = {'weights':tf.Variable(tf.random_normal([784, nodes_hidden_layer_1]))\n",
    "                     , 'bias':tf.Variable(tf.random_normal([nodes_hidden_layer_1]))}\n",
    "    \n",
    "    hidden_layer_2 = {'weights':tf.Variable(tf.random_normal([nodes_hidden_layer_1, nodes_hidden_layer_2]))\n",
    "                     , 'bias':tf.Variable(tf.random_normal([nodes_hidden_layer_2]))}\n",
    "    \n",
    "    hidden_layer_3 = {'weights':tf.Variable(tf.random_normal([nodes_hidden_layer_2, nodes_hidden_layer_3]))\n",
    "                     , 'bias':tf.Variable(tf.random_normal([nodes_hidden_layer_3]))}\n",
    "    \n",
    "    hidden_layer_4 = {'weights':tf.Variable(tf.random_normal([nodes_hidden_layer_3, nodes_hidden_layer_4]))\n",
    "                     , 'bias':tf.Variable(tf.random_normal([nodes_hidden_layer_4]))}\n",
    "    \n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([nodes_hidden_layer_4, num_classes]))\n",
    "                     , 'bias':tf.Variable(tf.random_normal([num_classes]))}\n",
    "    \n",
    "    '''Defining Computation'''\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(X,hidden_layer_1['weights']), hidden_layer_1['bias'])\n",
    "    #Applying activation function\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1,hidden_layer_2['weights']), hidden_layer_2['bias'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    layer_3 = tf.add(tf.matmul(layer_2,hidden_layer_3['weights']), hidden_layer_3['bias'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "    \n",
    "    layer_4 = tf.add(tf.matmul(layer_3,hidden_layer_4['weights']), hidden_layer_4['bias'])\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "    \n",
    "    output = tf.add(tf.matmul(layer_4,output_layer['weights']), output_layer['bias'])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining train Function\n",
    "\n",
    "def train(X):\n",
    "    \n",
    "    pred = neural_net(X)\n",
    "    \n",
    "    cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred,labels=Y))\n",
    "    '''Mininmizing cost function'''\n",
    "    # Using Gradient Descent\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost_function)\n",
    "    \n",
    "    \n",
    "    Num_Epochs = 3\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables()) # All it does is initialize the variables, weights and bias what we need to initialize\n",
    "        \n",
    "        for epoch in range(Num_Epochs):\n",
    "            loss = 0\n",
    "            for n in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                n, c = sess.run([optimizer, cost_function], feed_dict ={X:epoch_x,Y:epoch_y})\n",
    "                loss +=c #adding cost \n",
    "            print('Epoch ', epoch, 'out of ', Num_Epochs, 'loss: ', loss)\n",
    "        \n",
    "        # tf.argmax ---> returns index of the maximum value, below line is just verifying that we are getting no more than 1 as output because it is one hot encoded\n",
    "        check = tf.equal(tf.argmax(pred,1), tf.argmax(Y,1))\n",
    "        # tf.cast just changes to the dtype\n",
    "        accuracy = tf.reduce_mean(tf.cast(check, tf.float32))\n",
    "        print('Accuracy: ', accuracy.eval({X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tanaykarmarkar/anaconda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch  0 out of  3 loss:  12503837.7043\n",
      "Epoch  1 out of  3 loss:  2761520.9306\n",
      "Epoch  2 out of  3 loss:  1450771.19055\n",
      "Accuracy:  0.9254\n"
     ]
    }
   ],
   "source": [
    "train(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yayyyy!! We did pretty well, we scored 92% accuracy on the test set, that's not bad at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
