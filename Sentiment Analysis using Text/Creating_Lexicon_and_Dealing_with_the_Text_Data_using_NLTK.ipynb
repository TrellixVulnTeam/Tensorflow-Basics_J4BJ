{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Lexicon:\n",
    "\n",
    "Let suppose I have the dictionary as list with items ['fifa','world','cup','going','awesome']\n",
    "sentence = 'I am watching fifa world cup and it is going awesome'\n",
    ",\n",
    "then the sentence can be represented in terms of dictionary and numerics to deal with as follows:\n",
    "\n",
    "sentence vector representation:\n",
    "[1,1,1,1,1] as all the dictionary words are present in the sentence\n",
    "NLTK is a great way to do that as it has the dictionary that can be used \n",
    "for the bag of words we are aiming.\n",
    "\n",
    "tokenize:  sentence--->[I, am, watching, fifa, world, cup, and, it, is, going, awesome]\n",
    "lemmatize: watching, watch, watches---> basically watch, convert to form where it has\n",
    "           true meaning in the dictionary\n",
    "\n",
    "'''\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lines upto how many you want to read.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "Lines = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon(pos, neg):\n",
    "    lexicon = []\n",
    "    for file in [pos,neg]:\n",
    "        with open(file,'r') as f:\n",
    "            content =f.readlines()\n",
    "            for line in content[:Lines]:\n",
    "                all_words = word_tokenize(line.lower())\n",
    "                lexicon += list(all_words)\n",
    "    lexicon = [lemmatizer.lemmatize(n) for n in lexicon]\n",
    "    count_words = Counter(lexicon) #Counter counts how many times each word has repeated\n",
    "    \n",
    "    lexicon_final = []\n",
    "    for word in count_words:\n",
    "        if 1000 > count_words[word] > 50:\n",
    "            lexicon_final.append(word)\n",
    "            \n",
    "    return lexicon_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_sample(sample,lexicon, classification ):\n",
    "    \n",
    "    feature_set = []\n",
    "    \n",
    "    \n",
    "    with open(sample,'r') as f:\n",
    "        \n",
    "        content = f.readlines()\n",
    "        for line in content[:Lines]:\n",
    "            \n",
    "            sample_words = word_tokenize(line.lower())\n",
    "            sample_words = [lemmatizer.lemmatize(i) for i in sample_words]\n",
    "            features = np.zeros(len(lexicon))\n",
    "            for word in sample_words:\n",
    "                \n",
    "                if word.lower() in lexicon:\n",
    "                    idx = lexicon.index(word.lower())\n",
    "                    features[idx]+=1\n",
    "            features = list(features)\n",
    "            feature_set.append([features, classification])\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(pos, neg, test_size = 0.1):\n",
    "    \n",
    "    \n",
    "    Lexicon = lexicon(pos, neg)\n",
    "    features = []\n",
    "    features+=process_new_sample('pos.txt', Lexicon, [1,0])\n",
    "    features+=process_new_sample('neg.txt', Lexicon, [0,1])\n",
    "    \n",
    "    random.shuffle(features) #shuffling is important to train and test for numerous reasons\n",
    "    Test_Size = int(test_size*len(features))\n",
    "    \n",
    "    '''features-->\n",
    "    [[feature, label]]\n",
    "    \n",
    "    '''\n",
    "    features = np.array(features)\n",
    "    train_x = list(features[:,0][:-Test_Size])\n",
    "    train_y = list(features[:,1][:-Test_Size])\n",
    "    \n",
    "    test_x = list(features[:,0][-Test_Size:])\n",
    "    test_y = list(features[:,1][-Test_Size:])\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_x, train_y, test_x, test_y = create_labels('pos.txt','neg.txt')\n",
    "    with open('sentiment.pickle', 'wb') as f:\n",
    "        pickle.dump([train_x, train_y, test_x, test_y], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing is done! We have our pickle file.\n",
    "## In the next notebook, we will use this to train our neural network using tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
